

# In[1]:


from IPython.core.display import display, HTML

first_name = "Amir"
last_name = "Mohammadi"
student_number = "n11425806"

personal_header = "<h1>"+first_name+" "+last_name+" ("+student_number+")</h1>"
display(HTML(personal_header))


# ---

# <h1 align="center">Exploring Mental Health in the Tech Industry in 2016</h1>

# > ### *Data:* 
#    OSMI Mental Health in Tech Survey 2016
# > ### *Describtion:*
#    Currently over 1400 responses, the ongoing 2016 survey aims to measure attitudes towards mental health in the tech workplace, and examine the frequency of mental health disorders among tech workers.
# > ### *Source:*
#    https://www.kaggle.com/datasets/osmi/mental-health-in-tech-2016

# ---

# ### A. Importing all the necessary libraries
# It is of good practice to start with loading all the libraries that are needed troughout the analysis. 

# In[2]:


import pandas as pd
import numpy as np
import html
import os
import matplotlib.pyplot as plt
import seaborn as sn


# ### B. Setting up work directory

# In[3]:


os.getcwd() # to see what is set as current working directory


# In[4]:


# changing the work directory to another folder
os.chdir('/Users/amir/Documents/ifq619') # this folder should contain all the files we are working on for this project


# ---

# # 1. Question
# #### In the tech sector, which factors are most common for team member attitudes about mental health?

# We want to know what majority of tech workers feel or think about sharing their mental health issue (current or past)
# with their employers. Do they think their mental health will affect their productivity?

# # 2. Data

# ## 2.1 Loading the data and naming it
# Reading the file containing our dataset and calling it into a pandas object (simple and intuitive name is prefered)

# In[5]:


data = pd.read_csv("mental-heath-in-tech-2016_20161114.csv") # loading the data into notebook and naming it data


# ## 2.2 Displaying the data and intital visual inspection
# Before proceeding any further it is necessary to have a glance at the dataset to see what does it look like.

# In[6]:


data # just calling the name will return few rows and columns from the top and end of our table.


# #### We have 63 columns (features, questions) and 1433 rows (values - candidates, who answered the questions)

# In[7]:


type(data)


# ## 2.3 Cleaning the Data

# ### 2.3.1 Renaming the columns to short names
# Since most columns have really long names (the survey questions), it is a good idea to rplace them with short name, so it is easier and quicker to use them in codes.

# In[8]:


data.columns # looking at all the culomn names (questions in the survey)


# In[9]:


old_columns = data.columns # we have saved the original column names in an index for future reference.


# In[10]:


old_columns


# In[11]:


new_columns = ["self_employed", "number_of_employees", "company_in_tech", "role_in_tech", "company_mental_health",
                  "company_mental_health_options_awareness", "company_mental_health_discussion", "company_mental_health_resources", 
                   "company_mental_health_anonymity","company_mental_health_leave", "company_mental_health_discussion_impact", 
                   "company_physical_health_discussion_negative_impact", "mental_health_coworkers_discussion",
                   "mental_health_supervisor_discussion", "company_mental_health_serious", 
                   "company_mental_health_coworker_discussion_negative_impact", "mental_health_coverage", "mental_health_online_resources",
                  "mental_health_diagnosed_reveal", "mental_health_diagnosed_reveal_impact", "mental_health_diagnosed_coworker_reveal", 
                   "mental_health_diagnosed_coworker_reveal_impact_negative","mental_health_productivity_impact", 
                   "mental_health_productivity_impact_percent", "previous_companies", "previous_companies_mental_health_benefits", 
                   "previous_companies_mental_health_benefits_awareness","previous_companies_mental_health_discussion", 
                   "previous_companies_mental_health_resources", "previous_companies_mental_health_anonymity", 
                   "previous_companies_mental_health_discussion_negative_impact", 
                   "previous_companies_physical_health_discussion_negative_impact",
                  "previous_companies_mental_coworkers_health_discussion", "previous_companies_mental_supervisor_health_discussion", 
                   "previous_company_mental_health_serious", "previous_company_mental_health_coworker_discussion_negative_impact",
                  "future_company_physical_health_interview", "why/why_not", "future_company_mental_health_interview", "why/why_not2", 
                   "mental_health_negative_impact_career", "mental_health_negative_view_coworkers",
                  "mental_health_reveal_family&friends", "previous_and_current_company_mental_health_bad_response_experienced", 
                   "previous_and_current_company_mental_health_bad_response_experienced_others", "mental_health_family_history",
                  "mental_health_disorder_past", "mental_health_disorder_current", "yes:what_diagnosis", "maybe:what_diagnosis", 
                   "mental_health_disorder_professional_diagnosis","yes:condition_diagnosed", "mental_health_proffesional_treatment", 
                   "mental_health_treatment_affects_work", "mental_health_no_treatment_affects_work","age", "sex", "country_live", 
                   "live_us_territory", "country_work", "work_us_territory", "work_position", "remote"]


# In[12]:


len(new_columns) #making sure we have the right number of new columns as the original


# In[13]:


#putting the old and new columns side by side in a table for checking and future reference
old_new_columns = {"Old Column Names" : old_columns, "New Column Names" : new_columns}
old_new_columns = pd.DataFrame(old_new_columns)
old_new_columns


# In[14]:


old_new_columns


# In[15]:


data.columns = new_columns # replacing the old columns with new names


# In[16]:


data # visual inspection that the colmn name change went trough alright


# ### 2.3.2 Cleaing the *sex* column

# In[17]:


pd.set_option("display.max_rows", 70)
data.nunique().sort_values(ascending = False)
# to check if questions have received more unique answers than what they usually should


# The observation shows that the ***sex*** column has 70 unique values.  
# While we now it is better to have only three categories for better analysis.  
# Therefore we can extract all the unique values from sex column and re-assign them to male, female, other

# In[18]:


data["sex"] = data["sex"].str.lower().str.strip()
# changing all the values in the column to lower case and removing spaces from values.


# In[19]:


data.sex.unique() # all the 70 unique values in the sex column


# In[20]:


data.sex.unique() # returning all the unique values in the sex column


# In[21]:


# assigning all the possible male & female equivalents (from column sex) to new lists.
male = ["male", "m", "man", "male.", "androgynous", "male 9:1 female", "roughly", "male (cis)", "sex is male",
       "malr", "dude", "i'm a man why didn't you make this a drop down question. you should of asked sex? and i would of answered yes please. seriously how much text can this take?",
       "mail", "m|", "male/genderqueer", "male (trans, ftm)", "cisdude", "cis man"]

female = ["female", "i identify as female.", "female assigned at birth", "f", "woman", "fm", "cis female", "genderfluid (born female)", "female or multi-gender femme",
         "female/woman", "cisgender female", "genderqueer woman", "fem", "female (props for making this a freeform field, though)",
         "cis-woman", "genderflux demi-girl", "female-bodied; no feelings about gender", "transgender woman"]

other = list(data["sex"].value_counts().index)[2:]


# In[22]:


male


# In[23]:


female


# In[24]:


other


# In[25]:


# Replacing variants with male, female, other.
data["sex"] = data["sex"].replace(male, "male")

data["sex"] = data["sex"].replace(female, "female")

data["sex"] = data["sex"].replace(other, "other")


# In[26]:


data.sex.unique() # checking that replacement has worked


# In[27]:


data.sex.nunique()


# In[28]:


sex_values = data.sex.value_counts().sort_values(ascending=False).to_frame().reset_index()
sex_values.columns = ["Sex", "Count"]
sex_values


#  Now we can use the ***sex*** column in future analysis

# ### 2.3.3 Removing outliers from *age* column

# In[29]:


data.age.describe() # to see a summary of data in  the column


#  Although the average age seems alright, there are some extremes at both ends.  
#  We can solve this in two ways: 
#  1. Removing outliers by filtering the age column for a set min. and max. (risk loosing some data)
#  2. replacing the outliers value with the mean value (keeping all the data)

# In[30]:


# Calculating the mean age (excluding the outliers)
age_mean = data[(data["age"] >= 18) | (data["age"] <= 75)]["age"].mean()
age_mean


# In[31]:


# Replacing the outliers with mean
data["age"].replace(to_replace = data[(data["age"] < 18) | (data["age"] > 75)]
                    ["age"].tolist(), value = age_mean, inplace = True)


# In[32]:


data.age.describe() # checking that replacement has worked.


# ### 2.3.4 Finding and dealing with NaN (missing, not valid) values

# In[33]:


data.isna()


# In[34]:


# Finding the number of NaNs in each column.
data.isna().sum().sort_values(ascending = False)


# ##### We see that in some columns there are many Nan values, rendering those columns almost useless (or at least tricky to use)

# In[35]:


# We can remove the columns whre more than half of their values are NaN
total_nan_columns = (data.isna().sum() >= data.shape[0]/2).tolist()
total_nan_columns


# In[36]:


nan_columns = data.columns[total_nan_columns]
nan_columns


# In[37]:


dropped_columns = []
for column in nan_columns:
    dropped_columns.append(column)


# In[38]:


dropped_columns


# In[39]:


len(dropped_columns)


# In[40]:


# Removing all the columns with more than half the values NaN.
data.drop(labels = nan_columns, axis =1, inplace =True)


# In[41]:


data


# In[42]:


data.columns.to_list()


# ### 2.3.5 Changing column types to categorical
# 

# By looking at the data we see that only the ***age*** column is numerical and other columns contain categircal values.     
# Columns (questions) with only few possible answer can be changed to categorical datatype.

# In[43]:


for column in data.columns:
    if column != "age":
        data[column] = data[column].astype("category")


# In[44]:


data.dtypes


# In[ ]:





# ## 3. Analysis

# ### 3.1 Aggregation

# The first analysis can involve aggregating the data based on the ***sex*** column.

# In[45]:


data_grouped = data.groupby("sex") #grouping the data based on the gender values.


# In[46]:


data_grouped.mean() # to see and compare the average age of the participants by gender


# We see that the average age for both male and female participants is 34. This shows that sex column is reliable and can be used for further analysis.

# In[47]:


data.columns.sort_values()


# ## 4. Visualisation

# ### 4.1 Past history of mental health from gender-age point of view

# In[48]:


plt.figure(figsize=(14, 5))
sn.violinplot(data = data, y="age", x="mental_health_disorder_past", hue="sex", inner = "box")
plt.ylabel("Age")
plt.xlabel(old_new_columns.iloc[46,0])
plt.show()


# In[49]:


plt.figure(figsize=(14, 5))
(data["mental_health_disorder_past"].value_counts(normalize=True) * 100).plot(kind='bar',
                                                                              color = ["darkolivegreen", "yellowgreen", "grey"])
plt.ylabel("Percentage %")
plt.xlabel(old_new_columns.iloc[46,0])
plt.show()


# In[50]:


plt.figure(figsize=(14, 5))
(data["mental_health_proffesional_treatment"].value_counts(normalize=True) * 100).plot(kind="bar",
                                                                                       color = ["darkgoldenrod", "khaki"], edgecolor = "black")
plt.ylabel("Percentage %")
plt.xlabel(old_new_columns.iloc[52,0])
plt.show()


# In[51]:


plt.figure(figsize=(14, 5))
(data["mental_health_treatment_affects_work"].value_counts(normalize=True) * 100).plot(kind="bar",
                                                                                       color = ["darkred", "indianred", "lightsalmon", "peachpuff", "lightgrey"],
                                                                                      edgecolor = "black")
plt.ylabel("Percentage %")
plt.xlabel(old_new_columns.iloc[53,0])
plt.show()


# In[52]:


plt.figure(figsize=(14, 5))
(data["company_mental_health_discussion_impact"].value_counts(normalize=True) * 100).plot(kind="bar",
                                                                                         color = ["indigo", "orchid", "pink"])
plt.ylabel("Percentage %")
plt.xlabel(old_new_columns.iloc[10,0])
plt.show()


# This graph shows that although around 35% of employees feel confident about revealing their mental health issues with their employers, majority of the surveyed people (over 40%) are not sure. This can lead to employees hide their mental health issue and giving that it is a prevalent health concern (see graphs before) this can leave the employes untreated or neglected.

# ## 5. Insight

# The analysis shows that around 60% of participants reported a mental health issue in the past. However around 15% were reluctant to answer this question. The distribution of past mental health seems to be unifrom among the male and frmale workers with almost the same frequency. Non-binary gender participants are reporting significantly higher past mental issues. This could be either these people are more open and self-councsious about themselves or that society-induced pressures has a role.
# In regard to bein open to employers around 35% of employees feel confident about revealing their mental health issues, but majority of the surveyed people (over 40%) are not sure. This can lead to employees hide their mental health issue and giving that it is a prevalent health concern (see graphs before) this can leave the employees untreated or neglected.
# In general, I conclude that mental health is an issue of concern among tech workers and employers need to specifically address this by removing negative stigma from workplace and ensuring their employees that they will not be negatively judged if they seek treatment.

# ---

# <h1 align="center">A Million News Headlines</h1>

# > ### *Data:* 
#    A Million News Headlines
# > ### *Describtion:*
#    News headlines published over a period of 19 Years
# > ### *Source:*
#    https://www.kaggle.com/datasets/therohk/million-headlines

# ### A. Importing all the necessary libraries
# It is of good practice to start with loading all the libraries that are needed troughout the analysis. 

# In[53]:


import pandas as pd
import numpy as np
import html
import os
import matplotlib.pyplot as plt
import seaborn as sn
import collections
import nltk
from nltk.corpus import stopwords
from collections import Counter


# ### B. Setting up work directory

# In[54]:


os.getcwd() # to see what is set as current working directory


# In[55]:


# changing the work directory to another folder
os.chdir('/Users/amir/Documents/ifq619') # this folder should contain all the files we are working on for this project


# # 1. Question
# #### What can the headlines from the Australian national broadcaster (the ABC) tell us about the concerns of the Australian public over time?
# 

# Using this dataset we want to know what are the most common words in the news headlines from 2003 until the end of 2021.

# # 2. Data

# ## 2.1 Loading the data and naming it
# Reading the file containing our dataset and calling it into a pandas object (simple and intuitive name is prefered)

# In[56]:


headlines = pd.read_csv("abcnews-date-text.csv")


# ## 2.2 Displaying the data and intital visual inspection
# Before proceeding any further it is necessary to have a glance at the dataset to see what does it look like.

# In[57]:


headlines # just calling the name will return few rows and columns from the top and end of our table.


# We see that there only two columns and many many rows (good for analysis)

# In[58]:


headlines.shape


# In[59]:


# check if the columns are of the right data type.
headlines.dtypes


# The column ***publish_date*** is actually a date entry but is stored as integer.
# We can ask pandas to pars the date from this column and turn it to datetime type.

# In[60]:


# importing the dataset again with pars_dates argument.
headlines = pd.read_csv("abcnews-date-text.csv", parse_dates=["publish_date"])


# In[61]:


headlines.dtypes


# In[62]:


# lets check if there are any empty headlines
headlines.isnull().any().sum()


# In[63]:


#checking for duplicated values within the headlines column
headlines.duplicated().sum()


# In[64]:


headlines[headlines.duplicated()]


# visual inspection shows that these two headlines are not quite the same and they are three months apart.
# Therefore we treat them as not duplicates.

# # 3. Analysis

# ## 3.1 Articles length

# In[65]:


#checking to see how long the headlines are. The number of letters.
headlines["headline_text"].str.len().hist()


# In[66]:


#checking to see the average number of words for article
headlines["headline_text"].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist()
plt.show()


# ## 3.2 Extracting Date Elements

# Since we want to analyse the frequency of words over the years, it is a good idea to extract the year, month and day of each article into seperate columns.

# In[67]:


headlines["Year"] = headlines["publish_date"].dt.year
headlines["Month"] = headlines["publish_date"].dt.month
headlines["Day"] = headlines["publish_date"].dt.day


# In[68]:


headlines.head()


# In[69]:


# we do not need the original publish_date.
headlines.drop("publish_date", axis =1, inplace= True)


# In[70]:


headlines.dtypes


# In[71]:


headlines.set_index("Year")


# ### 3.3 Aggregating data

# To be able to analyse the data for month and day seperately we need to creat new dataframes from original data by grouping them based on the month and day.

# In[72]:


month_grouped = headlines.groupby(["Month"])["headline_text"].count()
day_grouped = headlines.groupby(["Day"])["headline_text"].count()


# In[73]:


year_grouped = headlines.groupby(["Year"])["headline_text"].count()


# In[74]:


month_grouped


# In[75]:


day_grouped


# In[76]:


year_grouped


# ### 3.4 Most common words

# For getting the most common words we will run into a problem as preposition words are the most common words and we do not want them.
# Therefore for filtering them we use function remove_stopwords

# In[77]:


common_words =Counter(" ".join(headlines["headline_text"].str.lower()).split()).most_common(100)


# In[78]:


common_words


# In[79]:


common_words = pd.DataFrame(common_words, columns = ["Word", "Count"])


# In[80]:


pd.set_option("display.max_rows", 100)
common_words


# In[81]:


common_words = common_words.sort_values(by = "Count", ascending = False)


# In[82]:


common_words.plot(kind="bar", figsize=(20,10),title=" Most Common Words")
plt.show()


# Since most of the frequent words are preposition word, we can manually select words that are valuable for our analysis.

# In[83]:


common_words_2 = common_words.iloc[[7,23,24,25,26,28,30,31,32,33,34,35,36,37,38,40,41,45,49,51,54,56,57,59,
                  60,63,65,66,71,72,73,75,77,78,79,80,81,82,86,89,95,96,97,98,99],]


# In[84]:


common_words_2 = common_words_2.reset_index(drop = True)
common_words_2


# Some words are repeated twice with different variant (govt:government, qld:queensland).
# We need to merge their counts under one word.

# In[85]:


common_words.iloc[2,1] = common_words.iloc[2,1] + common_words.iloc[20,1]
common_words.iloc[14,1] = common_words.iloc[14,1] + common_words.iloc[39,1]


# In[86]:


# and now we can delet the duplicate rows:
common_words.drop(labels=[20,39], axis=0, inplace = True)


# In[87]:


common_words_2 = common_words_2.sort_values(by = "Count", ascending = False)


# In[88]:


headlines.set_index("Year", inplace = True)


# # 4. Visualisation

# In[89]:


# Plotting the number of articles for each month of the year (2003-2021)
Mf = pd.Series(month_grouped)
Mf.plot(kind="line", figsize=(20,10),title=" Articles per each Month")
plt.xticks(np.arange(1,13,1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", 
                                      "Aug", "Sep", "Oct", "Nov", "Dec"])
plt.show()


# In[90]:


# Plotting the number of articles for each year (2003-2021)

Yf = pd.Series(year_grouped)
Yf.plot(kind="line", figsize=(20,10),title=" Articles per Year")
plt.xticks(year_grouped.index)
plt.show()


# In[91]:


# Plotting the number of articles for each day of the month (2003-2021)
Df = pd.Series(day_grouped)
Df.plot(kind="line", figsize=(20,10),title=" Articles per Day of the Month")
plt.show()


# In[92]:


# plotting the number of occurance for most common words over a period of 19 years
common_words_2.plot(kind="bar",  x= "Word", figsize=(20,10),title=" Most Common Words in ABC articles, 2003-2021")
plt.ylabel("Occurance")
plt.show()


# # 5. Insight

# From this analysis I tried to find out what are most topics discussed in ABC articles over a period of 19 years. The analysis shows that, in general, March sees the most number of articles published, while from December to Feburary next year, the number of published title plumet significantly. This could be assigned to the holiday season factor. In regard to days of the month, there is a uniform distribution of articles per each day, except for the last few days when there is a bit less articles published.
# The major change in areticle numbers is documented in yearly analysis. Although the total number of titles were increasing from 2003 until 2013, a sharp decline from then saw the number of articles shrink by two third in 2019.
# Analysis shows that first few most common words are related to police operations. I hope I am wrong and this is not a reflection of the bias in ABC toward special type of news. Fire also appears on the top list, and this is predictable after two years of devestating bush fires across the country.
